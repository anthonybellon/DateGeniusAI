{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0a6a37-1d43-466c-ae17-ccf14a266f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def find_and_replace_ambiguous_unicode(text, replacement=\"\"):\n",
    "    # Regex pattern to match non-ASCII characters\n",
    "    non_ascii_pattern = re.compile(r'[^\\x00-\\x7F]')\n",
    "    return non_ascii_pattern.sub(replacement, text)\n",
    "\n",
    "def clean_data(data, replacement=\"\"):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        cleaned_item = {k: find_and_replace_ambiguous_unicode(str(v), replacement) for k, v in item.items()}\n",
    "        cleaned_data.append(cleaned_item)\n",
    "    return cleaned_data\n",
    "\n",
    "def format_events_for_prompt(events):\n",
    "    formatted_events = [\n",
    "        {\n",
    "            \"ID\": event.get('ID', ''),\n",
    "            \"URL\": event.get('URL', ''),\n",
    "            \"Titre\": event.get('Titre', ''),\n",
    "            \"Description\": event.get('Description', ''),\n",
    "            \"Date de début\": event.get('Date de début', ''),\n",
    "            \"Date de fin\": event.get('Date de fin', ''),\n",
    "            \"Nom du lieu\": event.get('Nom du lieu', ''),\n",
    "            \"Adresse du lieu\": event.get('Adresse du lieu', ''),\n",
    "            \"Code postal\": event.get('Code postal', ''),\n",
    "            \"Ville\": event.get('Ville', ''),\n",
    "            \"Coordonnées géographiques\": event.get('Coordonnées géographiques', '')\n",
    "        }\n",
    "        for event in events\n",
    "    ]\n",
    "    return formatted_events\n",
    "\n",
    "def convert_and_clean_excel(input_file_path, output_file_path, formatted_output_file_path):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_file_path)\n",
    "\n",
    "    # Convert the DataFrame to a dictionary\n",
    "    data = df.to_dict(orient='records')\n",
    "\n",
    "    # Clean the data to remove ambiguous Unicode characters\n",
    "    cleaned_data = clean_data(data)\n",
    "\n",
    "    # Save the cleaned data to a JSON file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(cleaned_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Excel file has been cleaned and saved to {output_file_path}\")\n",
    "\n",
    "    # Format the events\n",
    "    formatted_events = format_events_for_prompt(cleaned_data)\n",
    "\n",
    "    # Save the formatted events to a new JSON file\n",
    "    with open(formatted_output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(formatted_events, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Formatted events have been saved to {formatted_output_file_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file_path = 'que-faire-a-paris-.xlsx'\n",
    "cleaned_output_file_path = 'cleaned_xlsx_data.json'\n",
    "formatted_output_file_path = 'formatted_xls_events.json'\n",
    "\n",
    "# Convert, clean, and format the Excel file\n",
    "convert_and_clean_excel(input_file_path, cleaned_output_file_path, formatted_output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7e0226-864d-43cf-843d-2adf0c6067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769b4de5-f1bd-4eb8-807b-0ade4f06db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "772024f4-64d3-4faa-9874-41cf1114c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Event: Suggested Event:\n",
      "ID: 63487, Title: \"L'Anne d'avant\" (The Year Before): an exhibition in the 10th arrondissement of Paris takes you right to the heart of French athletes' preparation., Date: 2024-07-26T02:00:00+02:00 to 2024-09-09T01:59:59+02:00, Location: TEP de la Grange-aux-Belles / Agnes Tirop, 8 rue Georg Friedrich Haendel, 75010, Paris\n",
      "\n",
      "This event might not be specifically about modern art, but it involves an exhibition related to French athletes' preparation, which could also be of interest if you\n"
     ]
    }
   ],
   "source": [
    "# Load formatted events from the JSON file\n",
    "def load_formatted_events(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "        \n",
    "# Clean HTML tags from text\n",
    "def clean_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, ' ', text) if isinstance(text, str) else text\n",
    "    \n",
    "# Preprocess events to create a TF-IDF matrix\n",
    "def preprocess_events(events):\n",
    "    for event in events:\n",
    "        event['combined_text'] = f\"{event['Titre']} {event['Description']} {event.get('Mots clés', '')}\"\n",
    "        event['combined_text'] = clean_html_tags(event['combined_text'])\n",
    "    texts = [event['combined_text'] for event in events]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix, events\n",
    "\n",
    "# Get top N relevant events based on query\n",
    "def get_top_relevant_events(query, vectorizer, tfidf_matrix, events, top_n=10):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    relevant_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return [events[idx] for idx in relevant_indices]\n",
    "\n",
    "# Generate the prompt with limited events\n",
    "def generate_prompt(query, relevant_events):\n",
    "    events_text = \"\\n\".join([\n",
    "        f\"ID: {event['ID']}, Title: {event['Titre']}, Date: {event['Date de début']} to {event['Date de fin']}, Location: {event['Nom du lieu']}, {event['Adresse du lieu']}, {event['Code postal']}, {event['Ville']}\" \n",
    "        for event in relevant_events\n",
    "    ])\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that helps people find events in Paris. Based on the query below, suggest the best matching event from the list.\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    Events:\n",
    "    {events_text}\n",
    "\n",
    "    Suggested Event:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Find the best matching event\n",
    "def find_event(query, events, vectorizer, top_n=10):\n",
    "    relevant_events = get_top_relevant_events(query, events, vectorizer, top_n)\n",
    "    prompt = generate_prompt(query, relevant_events)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that helps people find events in Paris.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Path to your formatted JSON file\n",
    "formatted_events_file_path = 'formatted_xlsx_events.json'\n",
    "\n",
    "# Load and preprocess the data\n",
    "formatted_events = load_formatted_events(formatted_events_file_path)\n",
    "vectorizer, tfidf_matrix, processed_events = preprocess_events(formatted_events)\n",
    "\n",
    "# Example usage\n",
    "user_query = \"I'd like to go to an expoisition on modern art\"\n",
    "suggested_event = find_event(user_query, vectorizer, tfidf_matrix, processed_events)\n",
    "print(\"Suggested Event:\", suggested_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8a800-b8df-48d1-9f57-e3d08ec706b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Integrate with a Flask application for deployment\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chatbot', methods=['POST'])\n",
    "def chatbot():\n",
    "    user_input = request.json.get('message')\n",
    "    response = generate_response(user_input)\n",
    "    return jsonify({'response': response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
